SONiC Mgmnt Repo:

This repository contains code for SONiC testbed deployment and setup, SONiC testing, test report processing.
The sonic-mgmt repository contains the scripts and tools for configuring the components to setup testbed topology.
The scripts and tools are based on ansible. 

Tetsbed Setup:

	We can Setup a virtual switch based testbed and deploy a T0 topology to it

	Install Ubuntu 20.04 , least 20GB of memory free

Download the sonic-vs:

	To run the tests with a virtual SONiC device, we need a virtual SONiC image

Setup sonic-mgmt docker:

	All testbed configuration steps and tests are run from a sonic-mgmt docker container. 
	This container has all the necessary packages and tools for SONiC testing

Setup VMs on the server:

Now we need to launch some VMs on the host to act as neighboring devices to our virtual SONiC switch

./testbed-cli.sh -m veos_vtb -n 4 -k vsonic start-vms server_1 password.txt

Deploy T0 topology:

	cd /data/sonic-mgmt/ansible
	./testbed-cli.sh -t vtestbed.yaml -m veos_vtb -k vsonic add-topo vms-kvm-t0 password.txt

	Note :testbed.csv is the topology configuration file for the testbed (Will be replaced by testbed.yaml)

Deploy minigraph on the DUT:

	./testbed-cli.sh -t vtestbed.yaml -m veos_vtb deploy-mg vms-kvm-t0 veos_vtb password.txt


sonic-mgmnt Repo:

 sonic-mgmnt
  |_ ansible 
  |
  |_ docs    [Details on each component and the directory structure]
  |
  |_ spytest [The SPyTest is a test automation framework designed to validate SONiC]
  |
  |_ test_reporting 
  |
  |_ test
      |_ all_scripts
	  |_ run_script
      |_ logs

Ansible​:
	Ansible is the main tool powering all the tasks for SONiC testing. The tasks include:

	Deploy and setup testbed

	Interact with various devices in testbed in ansible playbooks and in pytest scripts.

docs:
	Details on each component and the directory structure

Spytest​:
         The SPyTest is a test automation framework designed to validate SONiC​

	 This forms the core of the automation framework, 
         providing the necessary infrastructure and functionalities to author test scripts,
	 execute and generate test reports.

test_reporting 

	 For parsing, uploading and processing junit xml test reports generated by pytest. Processed test data is uploaded to Kusto for query

         (Kusto Query Language (KQL) is used to write queries in Azure Data Explorer, Azure Monitor Log Analytics, Azure Sentinel, and more)

Tests​
	Its contain diferent test features and test infrastructure code & test scripts
	
 	
	Additional info:
		Ansible playbooks are still used for testbed deployment and configuration. 
		All the customized ansible modules are re-used by the pytest scripts as well.

Pytest Repo:

	SPyTest is a test automation framework designed to validate SONiC. 

	It utilizes PyTest as its foundation and leverages various open-source Python packages for tasks such as device access, 
	CLI output parsing and traffic generation.

	Any reusable code needs to go under tests/common


tests
  |_ common  --- Connection, device( aos, arista, eos, cisco), dualtor(data_plane_utils, control_palne_utils,constant values )
		, fixtures(, Helpers(drop_counters , l2 l3 packet drop ),pkt filter(fillter_pkt_in_buffer),platformdevice_utils,interface_utils ,processor_utils,ssh_utils)
		  System utills,utillity
  |_ platform ---daemon_utils ,device_utils ,interface_utils, processor_utils, ssh_utils, transceiver_utils
  |_ ptftests ---
  |_ nat ------Purpose of the test is to verify a SONiC switch system correctly performs NAT translations based on configured rules
      |_ test_nat_bindings.py
      |_ files
           |_ all helpers for the nat feature
  |_ acl --Access-list (ACL) is a set of rules defined for controlling network traffic and reducing network attacks


Note : /data/sonic-mgmnt/tests/all feature test case, running testcase, collecting logs to /data/sonic-mgmnt/tests/logs

======================
Data Center:

There are rows of racks. Each rack has servers installed. Different tiers of network devices
The network device tiers are Tier0, Tier1 and Tier2.

T0 device, it usually sit on top of a rack.

For downstream, the T0 device is connected to all the servers in the rack. For upstream, the T0 device is connected to multiple T1 devices.

T1 device, it is usually connected to multiple downstream T0 devices and multiple upstream T2 devices.

T2 device, it is usually connected to multiple downstream T1 devices and multiple upstream Regional Gateways. 


================================
simulating neighbor devices and injecting/sniffing packets, the physical connection of testbed is designed like

The tests in this sonic-mgmt repository are mainly for testing features and functions of SONiC running as T0/T1/T2 network devices in data center. 
For that purpose, testbeds need to have the same topology of T0/T1/T2 devices

Key Components:

Test servers​

	PTF​  ---> Packet test framwork -- to sent and recieve the trffic 
		  The PTF docker container has BGP sessions established with VMs.

		  pytest fib fixture to generate routes to exabgp

		  exabgp ansible module to control exabgp service in PTF docker.

		  fib module generate http request to exabgp instance

		  support t0, t1, t1-lag

		  

   		  fib fixture _ > The fixture will detect testbed  type and then generate routes based on testbed type.

                  fixture will give some input data to tests

		  

		  supervisorctl ---> to check the exabgp service status

		  create, remove, restart exabgp service using supervisorctl

		  each exabgp instance listen to a http port for route announce/withdraw
		  
	VMs​ ---> The VMs can learn IP routes from PTF docker and populate the IP routes to DUT.

Open vSwitch​  ----> used as a vswitch in virtualized server environments.
		   
		    A vswitch forwards traffic between different VMs on the same physical host and 
		    also forwards traffic between VMs and the physical network.
		    The PTF docker, the VMs (Virtual Machine) and VLAN interfaces in test server can be interconnected by open vSwitch bridges

Fanout switches

	Root fanout switch (optional)
	The capability of PCIe switch chip set, really, is depending on its SoC. 
	The SoC in a fan-out switch chip set is designed to handle simple tasks, acting like a gateway between PCIe devices and the host. 
	The fan-out switch does not do anything special to the data but passing it to the end devices.
	One 'Fanout' layer-2 switch to connect all SONiC Switch ports and Linux server

	Leaf fanout switch​ 

SONiC DUT

Every DUT port is connected to one of leaf fanout switches.
Every leaf fanout switch has unique VLAN tag for every DUT port
Root fanout switch connects leaf fanout switches and test servers using 802.1Q trunks.
The root fanout switch is not mandatory if there is only one testbed or a test server is only used by one testbed.
Any test server can access any DUT port by sending a packet with the port VLAN tag 
(The root fanout switch should have this VLAN number enabled on the server trunk)

Through the VLAN tunnels of the fanout switches, the PTF docker and VMs can be connected to the ports of SONiC DUTs in flexible ways




================================



	
	Common:
		 cache - sonic-mgmt/tests/common/cache/facts_cache.py
		 
			we frequently need to gather facts from various devices again and again. 
			Most of the facts gatherings need to run some commands on remote devices through SSH connection, 
			parse the commands output and return the results
		 
				1.read(self, zone, key)
				2.write(self, zone, key, value)
				3.cleanup(self, zone=None)
				
				The FactsCache class has a dictionary for holding the cached facts in memory.
				we use local (sonic-mgmt container) pickle files to cache information
				
		 connections:
		 
					1.console connection of SONiC devices
					#For interactive shell
					# All supported console types
					# Console login via telnet (mad console)
					2.console_host
					3.ssh_console_conn
					4.telnet_console_conn
					
					
		 devices:
				   aos, arista, base, eos, cisco, duthost, fanout, ixia, juniper,
				   multi_asic, ptf, sonic, sonic_asic, vmhost
				   
				   Example: sonic
				   
							 A remote host running SONiC.

							This type of host contains information about the SONiC device (device info, services, etc.)
							and also provides the ability to run Ansible modules on the SONiC device.
		
				
		 dualtor:
		 
				This contain utills and constant values
				
				bsl_utils   ---> mock_transceiver_info_table (Replace the 'manufacturer' and 'model' fields for each port in the TRANSCEIVER_INFO table)
				constant  ----> contanst values
				control_palne_utils  --->functions used to verify control plane(APP_DB, STATE_DB) values
											1.dump db
											2.verify the db
											3.get_nbr_data
				data_plane_utils --->get_peerhost,arp_setup, validate_traffic_results, validate_long_distribution, verify and report
				
				dual_tor_common  ---> CalbleType, Active/standby port

				
				
				dual_tor_io ----> start_ptf_sniffer, stop_ptf_sniffer,genrate_vlan_server, genrate_soc_ip_map
								  start_io_test, genrate_downstream_traffic, genrate_upstream_traffic
								  fetch_captured_packets, sent_packets, get_test_results
				
				
				dual_tor_mock
				dual_tor_utils ---> Gets the PTF ports connected to the upper and lower ToR for the first T1
									get first server-facing interface on the ToR
									get ToR-facing interface on the PTF
									Gets the host object for the lower/upper ToR
									Gets the PTF portchannel ports connected to the T1 switchs.
									Gets the PTF ports connected to a given DUT for the first T1
									shutdown_fanout_upper_tor_intfs
									shutdown_fanout_lower_tor_intfs
									shutdown_fanout_upper_tor_intfs
									shutdown_t1_upper_tor_intfs
									shutdown_t1_lower_tor_intfs
								
								
				mux_simulaor_control ----> getting ip, port  and vmset_name of mux simulator server 
								  Mux status from mux simulator, restart_mux_simulator
								  
							
				nic_simulaor_control ----> Control utilities to interacts with nic_simulator
											gather nic_simulator related infomation
											stop nic_simulator service on the VM server host
											Setup connection to the nic_simulator.
				
				server_traffic_utils -----> Utils to verify traffic between ToR and server.
											Monit traffic between DUT and server.
				
				
				tunnel_traffic_utils ----> Tunnel traffic verification utilities
				
		 fixtures:
					1.advanced_reboot
					
					AdvancedReboot is used to perform reboot dut while running preboot/inboot operations

					This class collects information about the current testbed. This information is used by test cases to build
					inboot/preboot list. The class transfers number of config files to the dut/ptf in preparation for reboot test.
					
					Test cases can trigger test start utilizing runRebootTestcase API.
					
					2.connec_graph_facts : fanout_graph_facts, get_graph_facts
					
					3.duthost_utils:
									backup_and_restore_config_db - >Back up the existing config_db.json file and restore it once the test ends.
									backup and restore config_db.json on all duts
									Back up and restore config DB at the module level.
									Back up and restore config DB at the function level.
									
					4.fib_utils :
									get_t2_fib_info -Get parsed FIB information from redis DB for T2 topology
									get_fib_info - Get parsed FIB information from redis DB
									gen_fib_info_file - Store FIB info dumped & parsed from database to temporary file, then copy the file to PTF host
									
						
		 
		 helpers:
		 
			1.Drop_counters
					drop_counters  --> get_pkt_drops, Verify L3 drop counters, Verify L2 drop counters, verify_drop_counter
			        fanout_drop_counters --> get_trunk_port_to_server, get_openflow_port_id
		 
		    2.plaftform_api /scripts ---> chassis, component, fan, module, psu, psu_fan, thermal
			
			3.assertion,
			4.backend_acl
			5.dut_ports
			6.dut_utils
			7.generaters
			8.port_utils
			9.portchannel_to_vlan
			10.snmp_helper
			11.sonic_db
		
		 
		 ixia:
				
				common_helper
				ixia_fixtures
				ixia_helpers
				port
				
				
		 multibranch:
		 
					cli:
						auto_tech_support
						
		 pkt_filter: 
		 
					fillter_pkt_in_buffer
		 
		 platform:
		 
				daemon_utils --> check daemon running status inside pmon docker
				device_utils --> help us fanout switch operations
				interface_utils --> re-usable functions for checking status of interfaces on SONiC
				processor_utils -->re-usable functions for checking status of critical services.
								   all_critical_processes_healthy
								   check all critical processes. They should be all running.
								   keep on checking every 5 seconds until watch_secs drops below 0.
				ssh_utils --> ssh keys by generating ssh key on ptf host and adding this key to known_hosts on duthost
				transceiver_utils --> checking status of transceivers

				 
				
		 plugins
		 pytest_argus
		 snappi
		 storage_backend
		 
		 system_utills ---> docker.py --> docker contains utilities for interacting with Docker on the duthost
		 
		 template
	
	
	broadcom_data
	checkpoint
	cisco_data
	configs_reload
	constant
	errors
	port_toggle
	reboot
	testbed
	utilities


pytest Run:

	
https://github.com/sonic-net/sonic-mgmt/blob/de9f374f3de025bf8331a8a0d1654b1a1fc2b600/docs/tests/pytest.run.md?plain=1#L43



Testbed Inventory:

ansible/lab: Include all lab DUTs, fanout switches, and testbed server topologies

ansible/veos: all servers and VMs

ansible/testbed.csv is the topology configuration file for the testbed (Will be replaced by testbed.yaml)

ansible/testbed.yaml is the topology configuration file for the testbed

ansible/lab ansible/inventory  are the inventory files for the testbed
















